---
layout: post
title:  Post 2 Spectral Clustering
---

> This post will talk about a simple version of the *spectral clustering* algorithm for clustering data points. This post mainly focuses on the realization of the algorithm with python. The math behind it will only be briefly explained.



### Introduction


In this problem, we'll study *spectral clustering*. Spectral clustering is an unsupervised machine learning method for identifying meaningful parts of data sets with complex structure and grouping the similar data points without conern for the specific outcome. It is a powerful tool to deal with classification problem. However, in some sitations k-means clustering can classify data points very well, so spectral clustering is not necessary. Let's first import some useful packages.


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
from sklearn.metrics.pairwise import euclidean_distances
from scipy.optimize import minimize, rosen, rosen_der
from sklearn.cluster import KMeans
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

![output_2_1.png](/images/output_2_1.png)



K-means clustering is one of the simplest and most popular unsupervised machine learning algorithms. K-means algorithm identifies k number of centroids, and then allocate every data point to the nearest cluster, while keeping the centroids small. As we can see below, it performs well on circular blobs like these:


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![output_4_1.png](/images/output_4_1.png)


### Harder Clustering
However, for unevenly sized clusters or clusters that do not have the circular shape as seen above, K-means clustering will not be ideal.


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

![output_6_1.png](/images/output_6_1.png)


Now the two clusters aren't blobs anymore. Let's see how it performs.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![output_8_0.png](/images/output_8_0.png)



It looks like a quarter of the data points are misclassified. Spectral clustering can solve this problem. Let's derive spectral clustering and see how it performs with the moon-shape data points.

## Part A

A matrix that stores the location of the coordinates and the distance between them is a *similarity matrix* $$\mathbf{A}$$. $$\mathbf{A}$$ should be a `n` by `n` matrix, where `n` corresponds to the number of data points.

The matrix `A` I generated  contains information about how close the points are to each other. If two points `i` and `j` are within `epsilon` distance, then `A[i,j]` has a value of `1`. Otherwise, `A[i,j]` has a value of `0`. The distance between a point and itself is `0`, which is smaller than any positive `epsilon`, therefore the diagonal entries `A[i,i]` should all be equal to zero. When constructing the *similarity matrix* below, I used a parameter `epsilon`.



```python
def matrix_generator(input_data,epsilon):
    # First find the distance between vectors
    mx = euclidean_distances(input_data, input_data)
    # Turn the entries of the matrix that are smaller than user-defined distance threshold epsilon into 1's, other wise 0's
    mx = 1*(mx < epsilon)
    # Change the diagonal entries to 0s
    np.fill_diagonal(mx,0)

    return mx

# use epsilon = 0.4 to construct a similarity matrix
A = matrix_generator(X,0.4)
A
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])



## Part B

Let's talk about some math.

Let $$d_i = \sum_{j = 1}^n a_{ij}$$ be the sum of all entried on $$i$$th row of $$\mathbf{A}$$. This gives us the *degree* of $$i$$. We can understand it us how close a data point is to other data points.

Let $$C_0$$ and $$C_1$$ be two clusters of the data points. All of the data points belong to either $$C_0$$ or $$C_1$$.

Recall ealier, when I generated the moon-shape data, besided X, which contains the coordiates for the data, there was also a (1d `np.ndarray`) named y with shape `(1, n)`. Y contains values of  `0` and `1` and they are the labels for each data point. For example, if `y[i] = 1`, then point `i` belongs to cluster $$C_1$$.  

Let's define a function to compute *binary norm cut objective* of a matrix $$\mathbf{A}$$:

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, we have two a cut term and a volume term. Mathematically, they are defined as the following:
- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$

***Don't worry about it if you are a bit confused about the two terms above! I will talk about what they each represent. I will also demonstrate with a drawing example. before I go head and explain to you what cut term means, I just want to briefly say what the binary norm cut objective is for.***<br />

> A pair of clusters $$C_0$$ and $$C_1$$ is considered to be a classification of the data if we can minimize $$N_{\mathbf{A}}(C_0, C_1)$$. Let's dig this a bit deeper.


#### B.1 The Cut Term

First, the cut term $$\mathbf{cut}(C_0, C_1)$$ is the sum of $$a_{ij}$$ that has `i` and `j` in different clusters. If `i` and `j` are close (The distance between their coordinates is small, in our case, smaller than epsilon), $$a_{ij}$$. idealy, should have a value `0`, because we want them to be not near each other. In other words, if $$a_{ij}$$ has a value of `1`, this means that the boundry between the two distinct clusters is not clear. And we want the two clusters of points to be clearly separated and therefore, we want to small sum of $$a_{ij}$$.

Below is a function that computes the cut term given X and y.


```python
def cut(A,y):
    # Defind A_sum as 0 to begin with
    A_sum = 0
    # Write for loops to sum up the entries A[i,j] for each pair of points (i,j) in different clusters
    for i in range(len(A)):
        for j in range(len(A)):
            if y[i] != y[j]:
                # Avoid adding twice the value, for example, A[j,i] = A[i,j] and they both will be added, so we mutiply each adding term by 0.5 to avoid this situation
                A_sum = A_sum + 0.5*A[i,j]
    return A_sum
```

I demonstrated the cut function with X and y from the moon-shaped data I created earlier. In order to show indeed a good classifcation will have a smaller cum term (we actually have a perfect classification here since the data is generated synthetically) then a bad classification. I generated a random vector of random labels of length `n`, with each label equal to either 0 or 1. And run the cut function on with the random labels.


```python
y_fake = np.random.randint(2, size=200)
cut(A,y),cut(A,y_fake)
# As we can see the cut objective with true clusters gives us a much smaller value than the random ones.
```




    (13.0, 1116.0)



#### B.2 The Volume Term


Before we take a look at the second part in the norm cut objective. Quick reminder, earlier, we defined $$d_i = \sum_{j = 1}^n a_{ij}$$ be the $$i$$th row-sum of $$\mathbf{A}$$, which is also called the *degree* of $$i$$. You can think of the *degree* of $$i$$ to be how popular $$i$$ is, the more popular it is, the more close "neigbors" is has, and then the larger degree it has. Now, understanding the *volume term* should be easier.

The *volume* of cluster $$C_0$$ is a measure of how "big" cluster $$C_0$$ is. Recall, $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$. A bigger value for a cluster here indicates the sum of degree for each data points in this cluster is larger, which is equivalent to say how big the cluster is. One thing interesting here is that if a data point is misclassified into `(C_0)` , but it is actually among data points belong to another cluster`(C_1)`, when computing the $$\mathbf{vol}(C_0)$$ we will get a bigger value even though the contribution of the misclassified data point is wrong. But situation like this will result in a higher cut term, which can be understood as a way to punish situation like this. Therefore, in order to minimize the binary normcut objective, what we want to do is as such: correctly classify the data points into two groups and make the size of the two groups as big as possible.

1. Correctly classify the data points into two clusters.
2. Make the size of the two groups as big as possible.

I wrote a function called `vols(A,y)` which computes the volumes of $$C_0$$ and $$C_1$$, returning them as a tuple. Then I  wrote a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`.


```python
def vols(A,y):
    Sum_C1 = sum(A[y == 1].sum(axis = 1))
    Sum_C0 = sum(A[y == 0].sum(axis = 1))
    return (Sum_C0,Sum_C1)


def normcut(A,y):
    return cut(A,y) * (1/vols(A,y)[0] + 1/vols(A,y)[1])
```

Let's compare the `normcut` objective using both the true labels `y` and the fake labels. We will see a much smaller normcut value for the true labels.


```python
normcut(A,y),normcut(A,y_fake)
# The normcut for the true labels is much smaller compared to the norcut for the fake labels
```




    (0.011518412331615225, 0.995836336735081)

{::options parse_block_html="true" /}
<div class="got-help">

My classmate suggested to me that the vols function I wrote could benefit from adding more explanation in my own words. I totally agree with him/her. So I decided to add more comments and add a note for explaining the idea of the cut function and the vols function.

And another classmate pointed out that the following code I wrote before every single clustering plot I did should be avoided,
```Python
Labels_forcolor = np.where(labels < 0, 0, 1)
```
Instead, I could modify the function spectral_clustering to make it return labels directly so I can supply it as the color parameter inside of plt.scatter(). That was a good advice! This helped my code be more concise.

</div>
{::options parse_block_html="false" /}



#### Take a look of my mini dataset that contains only five data points.

![note.png](/images/note.png)


## Part C

We have now defined a normalized cut objective. How to find a vector `y` that contains all the labels and minimize the normalized cut objective for some data sets? one way to do it is trying for all combinations of `y` and to see which one gives us the smallest normalized cut objective. There are `2^n` different combinations of `y` possible. Computationally, this approach is quite not feasible when n gets large.

**A math trick can help us!**

Define a new vector $$$$\mathbf{z} \in \mathbb{R}^n$$$$ such that:

$$
z_i =
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\
\end{cases}
$$

Now, given `A` and `y`, we can transform the volume terms from last part into the $$\mathbf{z}$$ vector. We can obtain the binary norm cut objective of a matrix $$\mathbf{A}$$ by the following matrix product:



$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$


where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i$$ is the degree for each data point as defined earlier.



Let's write a function called `transform(A,y)` to construct the `z` vector given `A` and `y`.
After we constructed `z`, we can do the matrix product and verify it indeed is equal to the normcut objective.

And lastly, I am going to check the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. Let's first take a look at the first matrix multiplication, $$\mathbf{z}^T\mathbf{D}$$. Since $$\mathbf{D}$$ is a diagnol matrix, by multiplying the `z` vector with $$\mathbf{D}$$, we will get a sum of the following form:

$$$$\mathbf{z1}*\mathbf{d_{11}} + \mathbf{z2}*\mathbf{d_{22}} + \mathbf{z3}*\mathbf{d_{33}} ...$$$$

Recall, $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$.
Therefore, each `zi`*$$d_{ii}$$ gives the "proportion" of total degrees a single data point takes up in the cluster it belongs to. Since the sign of the vector $$\mathbf{z} \in \mathbb{R}^n$$ indicates the labeling of the cluster. $$\mathbf{z}^T\mathbf{D}\mathbb{1}$$ gives the linear combination of all the proportions (Sum them up). Having a sum of zero is equivalent to say that $$\mathbf{z}$$ should contain roughly as many positive as negative entries.


```python
# Write a function called transform(A,y) to compute the appropriate  𝐳  vector given A and y

def transform(A,y):
    y = np.where(y == 0, 1/vols(A,y)[0], y)
    y = np.where(y == 1, -1/vols(A,y)[1], y)
    return y


A_sum = A.sum(axis = 1)
D = np.diag(A_sum)
Z= transform(A,y)

# Check if the equation holds
np.isclose((Z.transpose()@(D - A)@Z)/(Z@D@Z),normcut(A,y))

```




    True




```python
# Check if Z.transpose()@D@np.ones(n) equals to zero
np.isclose(Z.transpose()@D@np.ones(n),0)
```




    True



## Part D

Now, we have transformed the original optimization problem of minimizing *binary norm cut objective* of a matrix $$\mathbf{A}$$ into minimizing the below function:

$$$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$$$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. As we can see from part C, a perfect classification will give as $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. To embedded this condition into our optimization equation, we can by substituting $$\mathbf{z}$$ with the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$.

The following code is for substituting $$\mathbf{z}$$ with the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$.


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n)

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

I used the `minimize` function with the method of truncated Newton algorithm from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$.


```python
z_ = minimize(orth_obj,Z,method='TNC')

```

**Note**: Recall in Part C, we specified that the entries of $$\mathbf{z}$$ should take only one of two values. Now the entries can have *any* value! We are no longer exactly optimizing the normcut objective, but rather an approximation. This is called the *continuous relaxation* of the normcut problem.

## Part E

The sign of the values or "labels" stored in `z_.x` actually contains information about the cluster labeling of data point `i`. I colored the points associated with a negative value and the points associated with a positve value differently.


The result is pretty amazing as you can!


```python
z_min = z_.x

np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)

# Color each data point, it is okay to do this here because the data is not that big yet
C_input = ["blue" if z_min[i]<0 else 'red'for i in range(len(z_min))]

# Plot
plt.scatter(X[:,0],X[:,1],c = C_input)

```


![output_30_0.png](/images/output_30_0.png)



## Part F

When we used the `minimize` function with the method of truncated Newton algorithm from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$ earlier, it was quite time consuming. If we are working on a much larger dataset, it will even be more time consuming. We can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices.

Our goal of optimization was:

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$.


The [Rayleigh-Ritz Theorem](https://en.wikipedia.org/wiki/Rayleigh%E2%80%93Ritz_method) states that the minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

Mutiplying the inverse of $$\mathbf{D}$$ both sides, we get the following:

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

$$\mathbb{1}$$ is the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$ and the vector $$\mathbf{z}$$ that we want must be the eigenvector with the *second*-smallest eigenvalue.

Construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. Find the eigenvector corresponding to its second-smallest eigenvalue, and I named it `z_eig`. Then, I plot the data using the sign of `z_eig` as the color.


```python
# Calculate Laplacian matrix of the similarity matrix A
L = np.linalg.inv(D) @ (D-A)

# Find eigenvector that is corresponding to the second smallest eigenvalue
Lam, U = np.linalg.eig(L)
ix = Lam.argsort()
z_eig = U[:,ix][:,1]


C_input = ["blue" if z_eig[i]<0 else 'red'for i in range(len(z_eig))]
plt.scatter(X[:,0],X[:,1],c = C_input)

```




![output_32_1.png](/images/output_32_1.png)



## Part G

To combine the discussion and results from previous parts, I wrote a function called `spectral_clustering(X, epsilon)`. I also demonstrated my function using the moon-shape generated before.



```python
def spectral_clustering(Data, epsilon):
    """
    The first input of the this function is a 2D array in which each row contains information for (x,y) coordinates for the data.
    The second input is the distance threshold epsilon.

    This function takes on the input data and categorize them into two different groups based on the user-defined distance threshold epsilon.

    The output of the function is a array contains only 1s and 0s. This array contains the binary labels indicating whether data point i is in group 0 and group 1.

    """

    # Construct the similarity matrix
    A = matrix_generator(Data,epsilon)
    A_sum = A.sum(axis = 1)
    D = np.diag(A_sum)
    # Construct the Laplacian matrix
    L = np.linalg.inv(D) @ (D-A)
    Lam, U = np.linalg.eig(L)
    # Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix
    ix = Lam.argsort()
    z_eig = U[:,ix][:,1]

    #Return labels based on this eigenvector
    Labels_forcolor = np.zeros(X.shape[0])
    Labels_forcolor[z_eig > 0] = 1
    return Labels_forcolor

```


```python
# The following is  an array of binary labels indicating whether data point i is in group 0 or group 1
Labels = spectral_clustering(X, 0.4)

# Demonstration through plotting
plt.scatter(X[:,0],X[:,1],c = Labels )


```


![output_35_0.png](/images/output_35_0.png)



## Part H

I generated three different data sets using `make_moons`, in which I increased numer of samples, `n` to `1000` and set noise to be `0.05`, `0.1` and `0.2`.


```python
# Generating different data sets using make_moons
np.random.seed(1234)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)

# The following gives an array of binary labels indicating whether data point i is in group 0 or group 1
labels = spectral_clustering(X, 0.4)

# Demonstration through plotting
plt.scatter(X[:,0],X[:,1],c = labels)

```



![output_37_0.png](/images/output_37_0.png)



Generate another dataset with noise = 0.1


```python
# Generating different data sets using make_moons
np.random.seed(1234)
n = 1000

X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.1, random_state=None)

# The following gives an array of binary labels indicating whether data point i is in group 0 or group 1
labels = spectral_clustering(X, 0.4)

# Demonstration through plotting
plt.scatter(X[:,0],X[:,1],c = labels)
```





![output_39_1.png](/images/output_39_1.png)



Generate another dataset with noise = 0.2


```python
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.2, random_state=None)

# The following gives an array of binary labels indicating whether data point i is in group 0 or group 1
labels = spectral_clustering(X, 0.4)

# Demonstration through plotting
plt.scatter(X[:,0],X[:,1],c = labels)
```


![output_41_1.png](/images/output_41_1.png)




As we can see from the three plots above, as noise increases, the clustering task is performed worse. If we keep increasing the noise, we will get an error saying "Singular Matrix". Singular matrix is not invertible and will become problematic because in the spectral_clustering I wrote, when generating the Laplacian matrix, we need to compute the inverse of the matrix.

## Part I

Let's try the spectral clustering function on another data set with a shape of eyes.


```python
np.random.seed(1111)
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])

# Epsilon = 0.4


# The following gives an array of binary labels indicating whether data point i is in group 0 or group 1
labels = spectral_clustering(X, 0.4)


# Demonstration through plotting
plt.scatter(X[:,0],X[:,1],c = labels)

```


![output_44_0.png](/images/output_44_0.png)




There are two concentric circles. As before, k-means will not do well here.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))


```


![output_46_0.png](/images/output_46_0.png)




Can your function successfully separate the two circles? Some experimentation here with the value of `epsilon` is likely to be required. Try values of `epsilon` between `0` and `1.0` and describe your findings. For roughly what values of `epsilon` are you able to correctly separate the two rings?


```python
# Epsilon = 0.1

np.random.seed(1111)

# The following gives an array of binary labels indicating whether data point i is in group 0 or group 1
labels = spectral_clustering(X, 0.1)

# Demonstration through plotting
plt.scatter(X[:,0],X[:,1],c = labels)

```

![output_48_0.png](/images/output_48_0.png)




```python
# Epsilon = 0.6

np.random.seed(1111)

# The following gives an array of binary labels indicating whether data point i is in group 0 or group 1
labels = spectral_clustering(X, 0.6)


# Demonstration through plotting
plt.scatter(X[:,0],X[:,1],c = labels)

```

![output_49_0.png](/images/output_49_0.png)

{::options parse_block_html="true" /}
<div class="gave-help">

I saw a classmate wrote the vols function as following,

```python
def vols(A,y):
    length, width = A.shape

    vol0 = sum([sum(A[i]) for i in range(length) if y[i] == 0])
    vol1 = sum([sum(A[i]) for i in range(length) if y[i] == 1])

    return (vol0, vol1)
```
I suggested this classmate to avoid using for loops inside of this function, and instead, using "numpy.sum()". For loop is more computationally costly.

And I saw another student put the docstrings in the middle of her function as if they were comments. I told her the following:

Python will only pick up a docstring if it is the first statement in the function body. Your functions will still run just fine, but I think your docstrings only serve as comments since you did not put in the very front of your function body. In this homework, you either put the docstring before your function or in between your function. For example, if you check the following for the spectral_clustering function you wrote in part G,

"spectral_clustering.__doc__ is None"

You will get,
"True".




</div>
{::options parse_block_html="false" /}
