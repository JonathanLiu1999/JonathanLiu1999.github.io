---
layout: post
title:  Post 3 Detecting Fake Newsss
---



In this blog post, I will demonstrate how to use Neural Network to build models to detect fake news with the help of TensorFlow.


In this blogpost, the data I build models with comes from the article,

>Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).




First, I need to import some helpful packages.


```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

# requires update to tensorflow 2.4
# >>> conda activate PIC16B
# >>> pip install tensorflow==2.4
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for embedding viz
import plotly.express as px
import plotly.io as pio
pio.templates.default = "plotly_white"

from nltk.corpus import stopwords

import nltk
nltk.download('stopwords')
```

**Acquire Training Data**

Each row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news, as determined by the authors of the paper above.


```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>




```python
def make_dataset(data_frame):
  stop = stopwords.words('english')
  data_frame[['title']] = data_frame['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  data_frame[['text']] = data_frame['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

  data = tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : data_frame[["title"]],
            "text"  : data_frame[["text"]]
        },
        {
            "fake"  : data_frame[["fake"]]
        }
    )
   )

  data = data.batch(100)
  return data


data = make_dataset(df)
```

```python
data = data.shuffle(buffer_size = len(data))

train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size)
val   = data.skip(train_size).take(val_size)

len(train), len(val)


```




    (180, 45)



The question I want to answer by building three models is the following,

> **When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?**

First, let's standardize and vectorize our data.

**Why do we need to do this?**

Computers do not understand strings! Before we vectorize the text (the process of representing text as a vector), we need to first deal with standardization. The process of standardization includes:

- Removing capitals

- Removing punctuation

- Removing HTML elements or other non-semantic content (especially needed when scraping text from some website)





```python
def standardization(input_data):
    # To ensure all letters are in lower case
    lowercase = tf.strings.lower(input_data)
    # To replace all the punctuations with space by using regular expression
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation


```

TensorFlow helps us deal with "empty word" and "unknown word" in a smart way. "Unknown workds" are the words that show up in testing dataset but not training dataset. Since our model is trained without seeing the testing dataset, so the model is not able to understand the words only show up in the test dataset. TensorFlow categorize all words like this as "unknown words". Now, we need to save the rank for the vectorization layer. By adapting the layer to the training data, the layer remembers the rank of the most common words. This allows us to perform frequency ranking on the titles.

**NOTE**

>Remember the keys of the dataset are stored in a tuple, and each element of tuple is a dictionary. We need to get the first element of the dictionary in the first element of the tuple. We need a vectorization layer for each input. I will write a function called `vectorization` to avoid repetitive code.


```python
# Vectorization
size_vocabulary = 2000

def vectorization(train,input):
  """
  dataset : a train dataset contains text
  input : a feature of the dataset that user wants to use predictors

  This function creates a vectorization layer by standardizing the text (The train text dataset user supplies) and vectorize them. The output of this function is a layer that is adapted to the training data. The layer remembers the rank of the most common words of the column of user's choice. This allows us to perform frequency ranking on the titles.

  """

  vectorize_layer = TextVectorization(
  standardize=standardization, # Use the standardization function we defined above
  max_tokens=size_vocabulary, # only consider 2000 most common words
  output_mode='int',
  output_sequence_length=500)
  vectorize_layer.adapt(train.map(lambda x, y: x[input]))
  return vectorize_layer

```

**Build a model contains only the title of the article**


Keras Functional API allows us to build models with more complicated structures. For Sequential API, we enumerate layers in the correct order and include them in our model. All the data would pass from one layer to the next and go through the same sequence of layers. However, for Functional API, we treat data inputs differently. For example, for images input, we could do a convolution layer, which would not be suitable for text input. In the dataset for this blog post, all data are strings, but because how media nowadays uses titles that do not reflect the content of the body of the articles to attract more readers. Therefore, treating the title and body of the articles to be different types of data is a reasonable and ideal approach.


First we need to create input for the model. The shape of the input is a tuple that corresponds to the column number of the data. Also, we need give a name (Should correspond to the name of the key in the dataset, so the model we build later on can make the connection to the data) and the type of the data for the input.


```python

# inputs

title_input = keras.Input(
    shape = (1,),
    name = "title",
    dtype = "string"
)


```

Now, we have the input, which is the foundation of our model. Let's build the model architecture to fit in the input in order to get a good output.

**Note**

>In the preparations below, we are actually not doing the computations. They only actually take place in the future when we actually run the model.


```python
# Data pipeline for the title input.

# Use the vectorization function we defined above to create a vectorized layer

vectorize_layer_title = vectorization(train,"title")
title_features = vectorize_layer_title(title_input)
# We now create the layer and then fit the layer with the title features from above
title_features = layers.Embedding(size_vocabulary, 10, name = "title_embedding")(title_features)
# The 20 percent drop out helps avoid overfitting
title_features = layers.Dropout(0.2)(title_features)
# Construct another layer
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
# Add another dense layer
title_features = layers.Dense(32,activation='relu')(title_features)
title_features = layers.Dense(2,name = "fake")(title_features)
```

Here comes the exciting part. Let's actually build the model!


```python
model_title = keras.Model(
    inputs = [title_input],
    outputs = title_features
)

model_title.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

# Save the history
history = model_title.fit(train,
                    validation_data=val,
                    epochs = 20)
```

    Epoch 1/20


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning:

    Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.



    180/180 [==============================] - 2s 8ms/step - loss: 0.6922 - accuracy: 0.5178 - val_loss: 0.6887 - val_accuracy: 0.5216
    Epoch 2/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.6820 - accuracy: 0.5882 - val_loss: 0.6136 - val_accuracy: 0.9304
    Epoch 3/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.5542 - accuracy: 0.8745 - val_loss: 0.3487 - val_accuracy: 0.9447
    Epoch 4/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.3071 - accuracy: 0.9383 - val_loss: 0.2017 - val_accuracy: 0.9499
    Epoch 5/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.1845 - accuracy: 0.9579 - val_loss: 0.1330 - val_accuracy: 0.9673
    Epoch 6/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.1373 - accuracy: 0.9651 - val_loss: 0.1068 - val_accuracy: 0.9713
    Epoch 7/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.1124 - accuracy: 0.9686 - val_loss: 0.0855 - val_accuracy: 0.9776
    Epoch 8/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0912 - accuracy: 0.9736 - val_loss: 0.0856 - val_accuracy: 0.9713
    Epoch 9/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0812 - accuracy: 0.9763 - val_loss: 0.0653 - val_accuracy: 0.9836
    Epoch 10/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0707 - accuracy: 0.9801 - val_loss: 0.0555 - val_accuracy: 0.9825
    Epoch 11/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0641 - accuracy: 0.9806 - val_loss: 0.0637 - val_accuracy: 0.9796
    Epoch 12/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0589 - accuracy: 0.9813 - val_loss: 0.0548 - val_accuracy: 0.9838
    Epoch 13/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0620 - accuracy: 0.9805 - val_loss: 0.0571 - val_accuracy: 0.9813
    Epoch 14/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0528 - accuracy: 0.9832 - val_loss: 0.0456 - val_accuracy: 0.9844
    Epoch 15/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0534 - accuracy: 0.9836 - val_loss: 0.0435 - val_accuracy: 0.9888
    Epoch 16/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0470 - accuracy: 0.9849 - val_loss: 0.0410 - val_accuracy: 0.9864
    Epoch 17/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0503 - accuracy: 0.9846 - val_loss: 0.0352 - val_accuracy: 0.9882
    Epoch 18/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0444 - accuracy: 0.9837 - val_loss: 0.0386 - val_accuracy: 0.9869
    Epoch 19/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0403 - accuracy: 0.9873 - val_loss: 0.0360 - val_accuracy: 0.9892
    Epoch 20/20
    180/180 [==============================] - 1s 7ms/step - loss: 0.0407 - accuracy: 0.9880 - val_loss: 0.0372 - val_accuracy: 0.9896


One thing I want to mention here is that originally, I set epochs to be 10, and from the loss and accuracy output, I see that the model is getting better and better while the Epoch is increasing. So, I decided to increase epochs from 10 to 20. One epoch means training the neural network with all the training data for one cycle. As we increase the number of epochs, the same number of times weights are changed in the neural network. And this makes the model go from underfitting to overfitting. Luckily, since the val_accuracy is still increasing when epoch reaches to 20, we are pretty confident that my model is not overfitting even when change epochs from 10 to 20.

Below is the visualization of the training history. We can see there is a slow increase in accuracy for both training and validation data while increasing the number of epoch.


```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
plt.savefig('title_history.png')
```
![title_history.png](/images/title_history.png)


**Build a model contains only the text body of the article**

I am not going to give much explanation for this one because the ideas of vectorizing the text, creating the input, making layers, building the model and eventually evaluate the model are very similar to the last part where I built a model with the article titles.


```python

# inputs

text_input = keras.Input(
    shape = (1,),
    name = "text",
    dtype = "string"
)

# The pipline for the text input

# Use the vectorization function we defined above to create a vectorized layer

vectorize_layer_text = vectorization(train,"text")
text_features = vectorize_layer_text(text_input)
# We now create the layer and then fit the layer with the title features from above
text_features = layers.Embedding(size_vocabulary, 10, name = "text_embedding")(text_features)
# The 20 percent drop out helps avoid overfitting
text_features = layers.Dropout(0.2)(text_features)
# Construct another layer
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
# Add another dense layer
text_features = layers.Dense(2,activation='relu',name = "fake")(text_features)

model_text = keras.Model(
    inputs = [text_input],
    outputs = text_features
)

model_text.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

history = model_text.fit(train,
                    validation_data=val,
                    epochs = 50)

from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
plt.savefig('text_history.png')
```
![text_history.png](/images/text_history.png)


    Epoch 1/50


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning:

    Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.



    180/180 [==============================] - 3s 15ms/step - loss: 0.6864 - accuracy: 0.5577 - val_loss: 0.6530 - val_accuracy: 0.6751
    Epoch 2/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.6365 - accuracy: 0.7666 - val_loss: 0.5691 - val_accuracy: 0.8553
    Epoch 3/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.5509 - accuracy: 0.8752 - val_loss: 0.4814 - val_accuracy: 0.9367
    Epoch 4/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.4633 - accuracy: 0.9075 - val_loss: 0.4010 - val_accuracy: 0.9411
    Epoch 5/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.3974 - accuracy: 0.9150 - val_loss: 0.3496 - val_accuracy: 0.9276
    Epoch 6/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.3415 - accuracy: 0.9228 - val_loss: 0.3042 - val_accuracy: 0.9427
    Epoch 7/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.3068 - accuracy: 0.9333 - val_loss: 0.2718 - val_accuracy: 0.9518
    Epoch 8/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.2727 - accuracy: 0.9430 - val_loss: 0.2459 - val_accuracy: 0.9538
    Epoch 9/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.2498 - accuracy: 0.9504 - val_loss: 0.2218 - val_accuracy: 0.9598
    Epoch 10/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.2281 - accuracy: 0.9509 - val_loss: 0.2032 - val_accuracy: 0.9656
    Epoch 11/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.2130 - accuracy: 0.9574 - val_loss: 0.1892 - val_accuracy: 0.9647
    Epoch 12/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1929 - accuracy: 0.9598 - val_loss: 0.1778 - val_accuracy: 0.9656
    Epoch 13/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1861 - accuracy: 0.9643 - val_loss: 0.1659 - val_accuracy: 0.9673
    Epoch 14/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1775 - accuracy: 0.9650 - val_loss: 0.1601 - val_accuracy: 0.9674
    Epoch 15/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.1641 - accuracy: 0.9686 - val_loss: 0.1517 - val_accuracy: 0.9711
    Epoch 16/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1586 - accuracy: 0.9679 - val_loss: 0.1457 - val_accuracy: 0.9698
    Epoch 17/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1496 - accuracy: 0.9680 - val_loss: 0.1400 - val_accuracy: 0.9728
    Epoch 18/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.1406 - accuracy: 0.9718 - val_loss: 0.1277 - val_accuracy: 0.9738
    Epoch 19/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1386 - accuracy: 0.9717 - val_loss: 0.1200 - val_accuracy: 0.9762
    Epoch 20/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1282 - accuracy: 0.9751 - val_loss: 0.1206 - val_accuracy: 0.9760
    Epoch 21/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1236 - accuracy: 0.9735 - val_loss: 0.1182 - val_accuracy: 0.9737
    Epoch 22/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1232 - accuracy: 0.9733 - val_loss: 0.1065 - val_accuracy: 0.9764
    Epoch 23/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1214 - accuracy: 0.9731 - val_loss: 0.1073 - val_accuracy: 0.9758
    Epoch 24/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1153 - accuracy: 0.9730 - val_loss: 0.1022 - val_accuracy: 0.9798
    Epoch 25/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1134 - accuracy: 0.9751 - val_loss: 0.1003 - val_accuracy: 0.9777
    Epoch 26/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.1062 - accuracy: 0.9749 - val_loss: 0.0925 - val_accuracy: 0.9776
    Epoch 27/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1024 - accuracy: 0.9772 - val_loss: 0.0906 - val_accuracy: 0.9820
    Epoch 28/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0987 - accuracy: 0.9767 - val_loss: 0.0909 - val_accuracy: 0.9771
    Epoch 29/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0956 - accuracy: 0.9774 - val_loss: 0.0882 - val_accuracy: 0.9809
    Epoch 30/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0929 - accuracy: 0.9780 - val_loss: 0.0853 - val_accuracy: 0.9816
    Epoch 31/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0888 - accuracy: 0.9807 - val_loss: 0.0755 - val_accuracy: 0.9847
    Epoch 32/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0874 - accuracy: 0.9806 - val_loss: 0.0846 - val_accuracy: 0.9786
    Epoch 33/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0875 - accuracy: 0.9804 - val_loss: 0.0751 - val_accuracy: 0.9824
    Epoch 34/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0850 - accuracy: 0.9794 - val_loss: 0.0703 - val_accuracy: 0.9844
    Epoch 35/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0831 - accuracy: 0.9816 - val_loss: 0.0733 - val_accuracy: 0.9831
    Epoch 36/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0801 - accuracy: 0.9805 - val_loss: 0.0737 - val_accuracy: 0.9802
    Epoch 37/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0768 - accuracy: 0.9813 - val_loss: 0.0685 - val_accuracy: 0.9849
    Epoch 38/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0695 - accuracy: 0.9853 - val_loss: 0.0686 - val_accuracy: 0.9849
    Epoch 39/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0771 - accuracy: 0.9828 - val_loss: 0.0662 - val_accuracy: 0.9849
    Epoch 40/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0692 - accuracy: 0.9836 - val_loss: 0.0679 - val_accuracy: 0.9836
    Epoch 41/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0698 - accuracy: 0.9837 - val_loss: 0.0671 - val_accuracy: 0.9849
    Epoch 42/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0704 - accuracy: 0.9831 - val_loss: 0.0684 - val_accuracy: 0.9824
    Epoch 43/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0686 - accuracy: 0.9840 - val_loss: 0.0685 - val_accuracy: 0.9834
    Epoch 44/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0659 - accuracy: 0.9847 - val_loss: 0.0698 - val_accuracy: 0.9813
    Epoch 45/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0663 - accuracy: 0.9823 - val_loss: 0.0574 - val_accuracy: 0.9871
    Epoch 46/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0662 - accuracy: 0.9839 - val_loss: 0.0537 - val_accuracy: 0.9867
    Epoch 47/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0599 - accuracy: 0.9843 - val_loss: 0.0574 - val_accuracy: 0.9867
    Epoch 48/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0574 - accuracy: 0.9864 - val_loss: 0.0567 - val_accuracy: 0.9876
    Epoch 49/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0591 - accuracy: 0.9857 - val_loss: 0.0537 - val_accuracy: 0.9878
    Epoch 50/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0595 - accuracy: 0.9869 - val_loss: 0.0489 - val_accuracy: 0.9896





Our model finally obtained a val_accuracy of 98.84 percent when I set Epoch to be 50, which is still worse than the first model, in which only 20 epochs were needed for a vaL_accuracy that is above 99 percent.

**Build a model using both the article title and the article text as input**




```python

# Data pipeline for the title input.

# Use the vectorization function we defined above to create a vectorized layer

vectorize_layer_title = vectorization(train,"title")
title_features = vectorize_layer_title(title_input)
# We now create the layer and then fit the layer with the title features from above
title_features = layers.Embedding(size_vocabulary, 10, name = "title_embedding")(title_features)
# The 20 percent drop out helps avoid overfitting
title_features = layers.Dropout(0.2)(title_features)
# Construct another layer
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
# Add another dense layer
title_features = layers.Dense(32,activation='relu')(title_features)

# The pipline for the text input

# Use the vectorization function we defined above to create a vectorized layer

vectorize_layer_text = vectorization(train,"text")
text_features = vectorize_layer_text(text_input)
# We now create the layer and then fit the layer with the title features from above
text_features = layers.Embedding(size_vocabulary, 3, name = "text_embedding")(text_features)
# The 20 percent drop out helps avoid overfitting
text_features = layers.Dropout(0.2)(text_features)
# Construct another layer
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
# Add another dense layer
text_features = layers.Dense(32,activation='relu')(text_features)
main = layers.concatenate([title_features, text_features], axis = 1)
main = layers.Dense(32, activation='relu')(main)
output = layers.Dense(2, name = "fake")(main)

finalmodel = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)


```

Take a look at the structure of the model.



```python
keras.utils.plot_model(finalmodel)
```
![model.png](/images/model.png)



The structure of the last model is more complicated than the two previous ones. Let's see if we will get a better prediction on the validation dataset.


```python
finalmodel.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)

history = finalmodel.fit(train,
                    validation_data=val,
                    epochs = 15)

from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
plt.savefig('final_history.png')
```
![final_history.png](/images/final_history.png)




    Epoch 1/15
    180/180 [==============================] - 4s 19ms/step - loss: 0.6786 - accuracy: 0.5484 - val_loss: 0.3682 - val_accuracy: 0.9060
    Epoch 2/15
    180/180 [==============================] - 3s 18ms/step - loss: 0.2839 - accuracy: 0.9153 - val_loss: 0.1440 - val_accuracy: 0.9618
    Epoch 3/15
    180/180 [==============================] - 3s 17ms/step - loss: 0.1513 - accuracy: 0.9566 - val_loss: 0.1000 - val_accuracy: 0.9739
    Epoch 4/15
    180/180 [==============================] - 3s 18ms/step - loss: 0.1155 - accuracy: 0.9673 - val_loss: 0.0756 - val_accuracy: 0.9780
    Epoch 5/15
    180/180 [==============================] - 3s 18ms/step - loss: 0.0750 - accuracy: 0.9793 - val_loss: 0.0464 - val_accuracy: 0.9882
    Epoch 6/15
    180/180 [==============================] - 3s 18ms/step - loss: 0.0530 - accuracy: 0.9868 - val_loss: 0.0280 - val_accuracy: 0.9937
    Epoch 7/15
    180/180 [==============================] - 3s 18ms/step - loss: 0.0358 - accuracy: 0.9895 - val_loss: 0.0197 - val_accuracy: 0.9951
    Epoch 8/15
    180/180 [==============================] - 3s 18ms/step - loss: 0.0253 - accuracy: 0.9927 - val_loss: 0.0138 - val_accuracy: 0.9967
    Epoch 9/15
    180/180 [==============================] - 3s 17ms/step - loss: 0.0205 - accuracy: 0.9947 - val_loss: 0.0139 - val_accuracy: 0.9969
    Epoch 10/15
    180/180 [==============================] - 3s 18ms/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.0069 - val_accuracy: 0.9980
    Epoch 11/15
    180/180 [==============================] - 3s 18ms/step - loss: 0.0144 - accuracy: 0.9959 - val_loss: 0.0093 - val_accuracy: 0.9978
    Epoch 12/15
    180/180 [==============================] - 3s 18ms/step - loss: 0.0132 - accuracy: 0.9960 - val_loss: 0.0032 - val_accuracy: 0.9998
    Epoch 13/15
    180/180 [==============================] - 3s 18ms/step - loss: 0.0100 - accuracy: 0.9971 - val_loss: 0.0048 - val_accuracy: 0.9984
    Epoch 14/15
    180/180 [==============================] - 3s 18ms/step - loss: 0.0116 - accuracy: 0.9966 - val_loss: 0.0045 - val_accuracy: 0.9982
    Epoch 15/15
    180/180 [==============================] - 3s 18ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0042 - val_accuracy: 0.9991



With the least number of epochs, my last model, in which both the article title and the article text are included as input, has the best performance on accuracy for both training and validation data. So I decided to this model as my final model and evaluate it on the testing data.


```python
# Read in the test data
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
df_test = pd.read_csv(test_url)
# Use the make_dataset function I previously wrote
data_test = make_dataset(df_test)
finalmodel.evaluate(data_test)
```

    225/225 [==============================] - 3s 11ms/step - loss: 0.0220 - accuracy: 0.9937





    [0.02201799303293228, 0.9937191009521484]



We got an amazing accuracy of 99.37 percent on the **unseen** data. This is very good!

**Embedding Visualization**

The basic ideas of word embedding is the following:

- Each word is represented as a vector in a vector space
- Words with similar meanings should be close to each other
- Pairs of words have similar relationship should be related to each other in similar directions



**What does PCA do?**

I constructed a ten dimensional embedding earlier, and this is impossible to visualize.
Principal component analysis is a good way to reduce the dimension of the data. To conduct PCA, we first compute the covariance matrix of the data and then find the loadings (another name for eigenvectors) of the covariance matrix. Ordering the eigenvectors by their corresponding eigenvalues from the largest to the smallest. The first principal component is the linear combination of the variates with weights (the coefficients in front of variates) equal to the entry of the eigenvector corresponds to the largest eigenvalue. It is the same idea of all other components. Each eigenvalue represents the variance explained by each component. We only need to retain the principal components that can explain most of the variance. In each loading, we can ignore the variates associated with a small weight (take absolute values of the coefficients) to achieve the goal of dimension reduction. In our case, we do not want to do so because we want to keep the top 2000 most frequent words (size_vocabulary = 2000). However, by taking the first two principal components (components explains the most variance of the original data), we are able to represent our data by only two columns.

```python
# Recall earlier, when adding layers to our models, I named the embedding layer, "title_embedding".
weights = finalmodel.get_layer('title_embedding').get_weights()[0] # get the weights from the embedding layer with the dimension we need
vocab = vectorize_layer_title.get_vocabulary()                # get the words that we vectorized in previous data precessing

# Use PCA to deduct dimension so we can visualize on 2D surface
from sklearn.decomposition import PCA
# Select 2D
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

# Plotly needs a dataframe input
embedding_df = pd.DataFrame({
    'word' : vocab,
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})

import plotly.express as px
fig = px.scatter(embedding_df,
                 x = "x0",
                 y = "x1",
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
fig.write_html("scatter.html")

```
{% include scatter.html %}

The pattern is not clear in the scatter plot I created, I did not expect to see the blob shape of my data. However, names like Hilary, Obama and Trump show up on the right end of the scatterplot. I think this makes sense because media like to create fake news about politics to attract more audience. Another interesting observation is I found the names of some counties to be on the fake news side on my plot and the names of some other countries are much more neutral, or close to the real news side of the plot.
